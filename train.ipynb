{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## This notebook uses data from the GAW to train a PersonCentric Webpage-classifier. \n",
    "\n",
    "This is performed on multiple data sets to investigate the accuracy depending on the training data, thus providing a way of evaluating the data set creation method."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# What is the problem, why is it hard?\n",
    "The Web, as representation of the physical world provides the opportunity to study large scale phenomena of entities and relations originating from structures of offline interactions.\n",
    "\n",
    "As stated by the IDC, unstructured data occupies approximately 80% of the digital space by volume compared to only 20% for structured data and continues to by the primary drive for data growth \\cite{potnis2019idc}.\n",
    "\n",
    "Investigating these interactions requires a prior information extraction process through which \\emph{entities}- and \\emph{relations}-centric \\emph{informational needs} are met \\cite{broder2002taxonomy, butt2015taxonomy}.\n",
    "\n",
    "This task becomes increasingly complex if the data is not available as structured data (i.e. RDF/XML) \\cite{gandhi2016information} but only as unstructured HTML/TEXT documents spread over millions of web pages.\n",
    "\n",
    "Therefore, a semantic enrichment process of unstructured web content is necessary to extract \\emph{entity}- and \\emph{relation}-centric information.\n",
    "\n",
    "With large data resources such a process \\emph{must} be performed automatically and in a shorter time frame, than the collection of the information by human-performed structuring, with given Web search opportunities.\n",
    "\n",
    "Despite being unstructured, web documents provide a structural and textual aspect of their content, which has been previously described in a combined representation \\cite{lanotte2017exploiting, fathi2004web}.\n",
    "\n",
    "The difficulty remains to be connecting these Web base structures to physical or organizational structures. In a first step towards this goal, we aim to provide the web data and network of the people associated within the different university web structures in Germany."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 4 GPU(s) available.\n",
      "We will use the GPU: GeForce GTX 1080 Ti\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "# If there's a GPU available...\n",
    "if torch.cuda.is_available():    \n",
    "    # Tell PyTorch to use the GPU.    \n",
    "    device = torch.device(\"cuda\")\n",
    "    print('There are %d GPU(s) available.' % torch.cuda.device_count())\n",
    "    print('We will use the GPU:', torch.cuda.get_device_name(0))\n",
    "    # If not...\n",
    "else:\n",
    "    print('No GPU available, using the CPU instead.')\n",
    "    device = torch.device(\"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = 42\n",
    "split_ratio_train_test = 0.8 \n",
    "split_ratio_train_val = 0.9 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "from collections import Counter\n",
    "import random as rd\n",
    "\n",
    "def load_raw_datasets(pathname):\n",
    "    df =  pd.read_csv(\"datasets/\" + pathname, delimiter='\\t', header=None, names=['sentence', 'label']).drop_duplicates().reset_index(drop=True)\n",
    "    return df\n",
    "\n",
    "# RegEx\n",
    "fromRegExTrue = load_raw_datasets(\"fromRegEx/train_true.tsv\")\n",
    "fromRegExFalse = load_raw_datasets(\"fromRegEx/train_false.tsv\")\n",
    "\n",
    "# Existing resource DBLP\n",
    "fromDBLPTrue = load_raw_datasets(\"fromDBLP/train_true.tsv\")\n",
    "\n",
    "# Manual annotation\n",
    "fromURLLabelTrue = load_raw_datasets(\"fromURLLabel/train_true.tsv\")\n",
    "fromURLLabelFalse = load_raw_datasets(\"fromURLLabel/train_false.tsv\")\n",
    "\n",
    "# Existing resource\n",
    "fromWikidata_on_seeds = load_raw_datasets(\"fromWikidata_urls_in_GAW/test_true.tsv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No duplicate sentences in all data sets\n"
     ]
    }
   ],
   "source": [
    "# Concatenate true and false samples from the individual sources, and remove internal duplicate sentences, remove cooccurring sample in RegEx and DBLP from URLLabel \n",
    "def dedup_by_sentences(dfList):\n",
    "    df_merged = pd.concat(dfList, ignore_index=True)\n",
    "    df_deduped = df_merged[df_merged.sentence.duplicated(keep=False) == False].reset_index(drop=True)\n",
    "    return df_deduped\n",
    "\n",
    "def dup_sentences(dfList):\n",
    "    dfunion = pd.concat(dfList,ignore_index=True)\n",
    "    duplicates = dfunion[dfunion.sentence.duplicated(keep=False)]\n",
    "    return duplicates\n",
    "\n",
    "def disjoint_samples(df1,df2):\n",
    "    df1_without_df2 = df1[~df1.sentence.isin(df2.sentence)]\n",
    "    df1_clean = df1_without_df2.sample(frac=1.0,random_state=seed).reset_index(drop=True)\n",
    "    return df1_clean\n",
    "\n",
    "## URLLabel merge true and false\n",
    "## remove internally duplicate sentences\n",
    "fromURLLabel_deduped = dedup_by_sentences([fromURLLabelTrue,fromURLLabelFalse])\n",
    "## DBLP\n",
    "fromDBLP_deduped = dedup_by_sentences([fromDBLPTrue])\n",
    "## RegEx \n",
    "fromRegEx_deduped = dedup_by_sentences([fromRegExTrue,fromRegExFalse])\n",
    "## Wikidata \n",
    "fromWikidata_deduped = dedup_by_sentences([fromWikidata_on_seeds])\n",
    "\n",
    "\n",
    "## Find duplicate sentences\n",
    "_duplicateDU = dup_sentences([fromDBLP_deduped,fromURLLabel_deduped])\n",
    "_duplicateDR = dup_sentences([fromDBLP_deduped,fromRegEx_deduped])\n",
    "_duplicateRU = dup_sentences([fromRegEx_deduped,fromURLLabel_deduped])\n",
    "\n",
    "## Wikidata dups\n",
    "_duplicateWD = dup_sentences([fromWikidata_deduped,fromDBLP_deduped])\n",
    "_duplicateWR = dup_sentences([fromWikidata_deduped,fromRegEx_deduped])\n",
    "_duplicateWU = dup_sentences([fromWikidata_deduped,fromURLLabel_deduped])\n",
    "\n",
    "\n",
    "## Deduplicated data sets\n",
    "_URLLabel_withoutDU = disjoint_samples(fromURLLabel_deduped,_duplicateDU)\n",
    "URLLabel_clean = disjoint_samples(_URLLabel_withoutDU,_duplicateWU)\n",
    "\n",
    "_DBLP_withoutDU = disjoint_samples(fromDBLP_deduped,_duplicateDU)\n",
    "DBLP_clean = disjoint_samples(_DBLP_withoutDU,_duplicateWD)\n",
    "\n",
    "_RegEx_withoutRU = disjoint_samples(fromRegEx_deduped,_duplicateRU)\n",
    "_RegEx_withoutRU_DR = disjoint_samples(_RegEx_withoutRU,_duplicateDR)\n",
    "RegEx_clean = disjoint_samples(_RegEx_withoutRU_DR,_duplicateWR)\n",
    "\n",
    "Wikidata_on_seeds_clean = disjoint_samples(disjoint_samples(fromWikidata_on_seeds,fromDBLP_deduped),fromRegEx_deduped)\n",
    "\n",
    "assert all(pd.concat([RegEx_clean,DBLP_clean,URLLabel_clean,Wikidata_on_seeds_clean]).reset_index(drop=True).duplicated() == False) == True\n",
    "# assert all(pd.concat([RegEx_clean,DBLP_clean,URLLabel_clean,fromWikidata_on_seeds]).reset_index(drop=True).duplicated() == False) == True\n",
    "print(\"No duplicate sentences in all data sets\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "URLLabel_clean_counter: Counter({0: 1407, 1: 605})\n",
      "false_true_ratio:  2.3256198347107437\n",
      "URLLabel_test_counter: Counter({0: 281, 1: 121})\n",
      "Counter({1: 1790})\n",
      "DBLP_test_counter: Counter({0: 281, 1: 121})\n",
      "Wikidata_test_counter: Counter({0: 589, 1: 254})\n",
      "Counter({1: 254})\n",
      "URLLabel_train_counter: Counter({0: 1126, 1: 484})\n",
      "DBLP_train_counter: Counter({0: 3881, 1: 1669})\n",
      "RegEx_train_counter: Counter({0: 11260, 1: 4840})\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/lib/python3/dist-packages/ipykernel_launcher.py:48: UserWarning: Boolean Series key will be reindexed to match DataFrame index.\n"
     ]
    }
   ],
   "source": [
    "# Construction of train, test dataset \n",
    "\n",
    "def construct_test_set(df_for_true, df_for_false, sampleing_true, sampleing_false):\n",
    "    test_true = df_for_true[df_for_true.label == 1].sample(**sampleing_true).reset_index(drop=True)\n",
    "    test_false = df_for_false[df_for_false.label == 0].sample(**sampleing_false).reset_index(drop=True)\n",
    "    test_df = pd.concat([test_true, test_false],ignore_index=True).reset_index(drop=True)\n",
    "    return test_df\n",
    "\n",
    "def construct_train_set(df, testsamples):\n",
    "    df_train = df[~df.sentence.isin(testsamples.sentence)]\n",
    "    return df_train\n",
    "\n",
    "URLLabel_clean_counter = Counter(URLLabel_clean.label)\n",
    "print(\"URLLabel_clean_counter:\",URLLabel_clean_counter)\n",
    "# Counter({0: 1407, 1: 606}) \n",
    "false_true_ratio = URLLabel_clean_counter[0]/URLLabel_clean_counter[1]\n",
    "print(\"false_true_ratio: \", false_true_ratio)\n",
    "\n",
    "## Test dataset\n",
    "# URLLabel \n",
    "URLLabel_test = construct_test_set(URLLabel_clean,URLLabel_clean,{\"frac\": 1-split_ratio_train_test,\"random_state\" : seed},{\"frac\": 1-split_ratio_train_test,\"random_state\" : seed})\n",
    "URLLabel_test_counter = Counter(URLLabel_test.label)\n",
    "print(\"URLLabel_test_counter: \" + str(URLLabel_test_counter))\n",
    "\n",
    "# DBLP/RegEx\n",
    "DBLP_test = construct_test_set(DBLP_clean, RegEx_clean, {\"n\": URLLabel_test_counter[1],\"random_state\" : seed},{\"n\": URLLabel_test_counter[0],\"random_state\" : seed})\n",
    "DBLP_test_counter = Counter(DBLP_test.label)\n",
    "DBLP_clean_counter = Counter(DBLP_clean.label)\n",
    "print(DBLP_clean_counter)\n",
    "print(\"DBLP_test_counter: \" + str(DBLP_test_counter))\n",
    "\n",
    "# Wikidata\n",
    "n_wikidata = len(Wikidata_on_seeds_clean)\n",
    "Wikidata_test = construct_test_set(Wikidata_on_seeds_clean,RegEx_clean, {\"n\": n_wikidata,\"random_state\" : seed},{\"n\": int(n_wikidata*URLLabel_test_counter[0]/URLLabel_test_counter[1]),\"random_state\" : seed})\n",
    "Wikidata_test_counter = Counter(Wikidata_test.label)\n",
    "print(\"Wikidata_test_counter: \" + str(Wikidata_test_counter))\n",
    "Wikidata_on_seeds_clean_counter = Counter(Wikidata_on_seeds_clean.label)\n",
    "print(Wikidata_on_seeds_clean_counter)\n",
    "\n",
    "## Train dataset\n",
    "# URLLabel\n",
    "URLLabel_train = URLLabel_clean[~URLLabel_clean.sentence.isin(URLLabel_test.sentence)]\n",
    "URLLabel_train_counter = Counter(URLLabel_train.label)\n",
    "print(\"URLLabel_train_counter: \"+ str(URLLabel_train_counter))\n",
    "\n",
    "#DBLP/RegEx\n",
    "DBLP_train_true = DBLP_clean[~DBLP_clean.sentence.isin(DBLP_test.sentence)]\n",
    "DBLP_train_false = RegEx_clean[RegEx_clean.label == 0][~RegEx_clean.sentence.isin(DBLP_test.sentence)].sample(int(len(DBLP_train_true)*false_true_ratio),random_state=seed).reset_index(drop=True)\n",
    "DBLP_train = pd.concat([DBLP_train_true, DBLP_train_false],ignore_index=True).reset_index(drop=True)\n",
    "DBLP_train_counter = Counter(DBLP_train.label)\n",
    "print(\"DBLP_train_counter: \" + str(DBLP_train_counter))\n",
    "\n",
    "# Remaining sample from the RegEx\n",
    "#RegEx\n",
    "remaining_RegEx = RegEx_clean[~RegEx_clean.sentence.isin(DBLP_train.sentence)]\n",
    "RegEx_train_true = remaining_RegEx[remaining_RegEx.label == 1].sample(10*URLLabel_train_counter[1],random_state=seed)\n",
    "RegEx_train_false = remaining_RegEx[remaining_RegEx.label == 0].sample(10*URLLabel_train_counter[0],random_state=seed)\n",
    "RegEx_train = pd.concat([RegEx_train_true, RegEx_train_false],ignore_index=True).sample(frac=1,random_state=seed).reset_index(drop=True)\n",
    "RegEx_train_counter = Counter(RegEx_train.label)\n",
    "print(\"RegEx_train_counter: \"+ str(RegEx_train_counter))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def formatDataset(df, n0, n1, seed):\n",
    "    df0 = df[df['label'] == 0].sample(n0,random_state=seed)\n",
    "    df1 = df[df['label'] == 1].sample(n1,random_state=seed)\n",
    "    return pd.concat([df0,df1],ignore_index=True).reset_index(drop=True)\n",
    "\n",
    "def traintestDict(df1, df2List):\n",
    "    return {\"train\":df1.sample(frac=1,random_state=seed).reset_index(drop=True), \n",
    "          \"test\":[df2.sample(frac=1,random_state=seed).reset_index(drop=True) for df2 in df2List]}\n",
    "\n",
    "def printCounter(dataset):\n",
    "    return Counter(dataset[\"train\"].label), [Counter(df.label) for df in dataset[\"test\"]]\n",
    "\n",
    "testset_list = [URLLabel_test, DBLP_test, Wikidata_test]\n",
    "\n",
    "M_dataset = traintestDict(URLLabel_train, testset_list)\n",
    "D_dataset = traintestDict(DBLP_train, testset_list)\n",
    "R_dataset = traintestDict(RegEx_train, testset_list)\n",
    "\n",
    "\n",
    "M_counter = printCounter(M_dataset)\n",
    "D_counter = printCounter(D_dataset)\n",
    "R_counter = printCounter(R_dataset)\n",
    "\n",
    "\n",
    "print(printCounter(M_dataset))\n",
    "print(printCounter(D_dataset))\n",
    "print(R_counter)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
