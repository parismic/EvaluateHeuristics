{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## This notebook uses data from the GAW to train a PersonCentric Webpage-classifier. \n",
    "\n",
    "This is performed on multiple data sets to investigate the accuracy depending on the training data, thus providing a way of evaluating the data set creation method."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# What is the problem, why is it hard?\n",
    "The Web, as representation of the physical world provides the opportunity to study large scale phenomena of entities and relations originating from structures of offline interactions.\n",
    "\n",
    "As stated by the IDC, unstructured data occupies approximately 80% of the digital space by volume compared to only 20% for structured data and continues to by the primary drive for data growth \\cite{potnis2019idc}.\n",
    "\n",
    "Investigating these interactions requires a prior information extraction process through which \\emph{entities}- and \\emph{relations}-centric \\emph{informational needs} are met \\cite{broder2002taxonomy, butt2015taxonomy}.\n",
    "\n",
    "This task becomes increasingly complex if the data is not available as structured data (i.e. RDF/XML) \\cite{gandhi2016information} but only as unstructured HTML/TEXT documents spread over millions of web pages.\n",
    "\n",
    "Therefore, a semantic enrichment process of unstructured web content is necessary to extract \\emph{entity}- and \\emph{relation}-centric information.\n",
    "\n",
    "With large data resources such a process \\emph{must} be performed automatically and in a shorter time frame, than the collection of the information by human-performed structuring, with given Web search opportunities.\n",
    "\n",
    "Despite being unstructured, web documents provide a structural and textual aspect of their content, which has been previously described in a combined representation \\cite{lanotte2017exploiting, fathi2004web}.\n",
    "\n",
    "The difficulty remains to be connecting these Web base structures to physical or organizational structures. In a first step towards this goal, we aim to provide the web data and network of the people associated within the different university web structures in Germany."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 4 GPU(s) available.\n",
      "We will use the GPU: GeForce GTX 1080 Ti\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "# If there's a GPU available...\n",
    "if torch.cuda.is_available():    \n",
    "    # Tell PyTorch to use the GPU.    \n",
    "    device = torch.device(\"cuda\")\n",
    "    print('There are %d GPU(s) available.' % torch.cuda.device_count())\n",
    "    print('We will use the GPU:', torch.cuda.get_device_name(0))\n",
    "    # If not...\n",
    "else:\n",
    "    print('No GPU available, using the CPU instead.')\n",
    "    device = torch.device(\"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = 42\n",
    "split_ratio_train_test = 0.8 \n",
    "split_ratio_train_val = 0.9 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from collections import Counter\n",
    "\n",
    "\n",
    "def load_raw_datasets(pathname):\n",
    "    df =  pd.read_csv(\"datasets/\" + pathname, delimiter='\\t', header=None, names=['sentence', 'label']).drop_duplicates().reset_index(drop=True)\n",
    "    return df\n",
    "\n",
    "# RegEx\n",
    "fromRegExTrue = load_raw_datasets(\"fromRegEx/train_true.tsv\")\n",
    "fromRegExFalse = load_raw_datasets(\"fromRegEx/train_false.tsv\")\n",
    "\n",
    "# Existing resource DBLP\n",
    "fromDBLPTrue = load_raw_datasets(\"fromDBLP/train_true.tsv\")\n",
    "\n",
    "# Manual annotation\n",
    "fromURLLabelTrue = load_raw_datasets(\"fromURLLabel/train_true.tsv\")\n",
    "fromURLLabelFalse = load_raw_datasets(\"fromURLLabel/train_false.tsv\")\n",
    "\n",
    "# Existing resource\n",
    "fromWikidata_on_seeds = load_raw_datasets(\"fromWikidata_urls_in_GAW/test_true.tsv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No duplicate sentences in all data sets\n"
     ]
    }
   ],
   "source": [
    "# Concatenate true and false samples from the individual sources, and remove internal duplicate sentences, remove cooccurring sample in RegEx and DBLP from URLLabel \n",
    "def dedup_by_sentences(dfList):\n",
    "    df_merged = pd.concat(dfList, ignore_index=True)\n",
    "    df_deduped = df_merged[df_merged.sentence.duplicated(keep=False) == False].reset_index(drop=True)\n",
    "    return df_deduped\n",
    "\n",
    "def dup_sentences(dfList):\n",
    "    dfunion = pd.concat(dfList,ignore_index=True)\n",
    "    duplicates = dfunion[dfunion.sentence.duplicated(keep=False)]\n",
    "    return duplicates\n",
    "\n",
    "def disjoint_samples(df1,df2):\n",
    "    df1_without_df2 = df1[~df1.sentence.isin(df2.sentence)]\n",
    "    df1_clean = df1_without_df2.sample(frac=1.0,random_state=seed).reset_index(drop=True)\n",
    "    return df1_clean\n",
    "\n",
    "## URLLabel merge true and false\n",
    "## remove internally duplicate sentences\n",
    "fromURLLabel_deduped = dedup_by_sentences([fromURLLabelTrue,fromURLLabelFalse])\n",
    "## DBLP\n",
    "fromDBLP_deduped = dedup_by_sentences([fromDBLPTrue])\n",
    "## RegEx \n",
    "fromRegEx_deduped = dedup_by_sentences([fromRegExTrue,fromRegExFalse])\n",
    "## Wikidata \n",
    "fromWikidata_deduped = dedup_by_sentences([fromWikidata_on_seeds])\n",
    "\n",
    "\n",
    "## Find duplicate sentences\n",
    "_duplicateDU = dup_sentences([fromDBLP_deduped,fromURLLabel_deduped])\n",
    "_duplicateDR = dup_sentences([fromDBLP_deduped,fromRegEx_deduped])\n",
    "_duplicateRU = dup_sentences([fromRegEx_deduped,fromURLLabel_deduped])\n",
    "\n",
    "## Wikidata dups\n",
    "_duplicateWD = dup_sentences([fromWikidata_deduped,fromDBLP_deduped])\n",
    "_duplicateWR = dup_sentences([fromWikidata_deduped,fromRegEx_deduped])\n",
    "_duplicateWU = dup_sentences([fromWikidata_deduped,fromURLLabel_deduped])\n",
    "\n",
    "\n",
    "## Deduplicated data sets\n",
    "_URLLabel_withoutDU = disjoint_samples(fromURLLabel_deduped,_duplicateDU)\n",
    "URLLabel_clean = disjoint_samples(_URLLabel_withoutDU,_duplicateWU)\n",
    "\n",
    "_DBLP_withoutDU = disjoint_samples(fromDBLP_deduped,_duplicateDU)\n",
    "DBLP_clean = disjoint_samples(_DBLP_withoutDU,_duplicateWD)\n",
    "\n",
    "_RegEx_withoutRU = disjoint_samples(fromRegEx_deduped,_duplicateRU)\n",
    "_RegEx_withoutRU_DR = disjoint_samples(_RegEx_withoutRU,_duplicateDR)\n",
    "RegEx_clean = disjoint_samples(_RegEx_withoutRU_DR,_duplicateWR)\n",
    "\n",
    "Wikidata_on_seeds_clean = disjoint_samples(disjoint_samples(fromWikidata_on_seeds,fromDBLP_deduped),fromRegEx_deduped)\n",
    "\n",
    "assert all(pd.concat([RegEx_clean,DBLP_clean,URLLabel_clean,Wikidata_on_seeds_clean]).reset_index(drop=True).duplicated() == False) == True\n",
    "# assert all(pd.concat([RegEx_clean,DBLP_clean,URLLabel_clean,fromWikidata_on_seeds]).reset_index(drop=True).duplicated() == False) == True\n",
    "print(\"No duplicate sentences in all data sets\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Construction of train, test dataset \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "URLLabel_clean_counter: Counter({0: 1407, 1: 605})\n",
      "false_true_ratio:  2.3256198347107437\n",
      "URLLabel_test_counter: Counter({0: 281, 1: 121})\n",
      "Counter({1: 1790})\n",
      "DBLP_test_counter: Counter({0: 281, 1: 121})\n",
      "Wikidata_test_counter: Counter({0: 589, 1: 254})\n",
      "Counter({1: 254})\n",
      "URLLabel_train_counter: Counter({0: 1126, 1: 484})\n",
      "DBLP_train_counter: Counter({0: 3881, 1: 1669})\n",
      "RegEx_train_counter: Counter({0: 11260, 1: 4840})\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/lib/python3/dist-packages/ipykernel_launcher.py:46: UserWarning: Boolean Series key will be reindexed to match DataFrame index.\n"
     ]
    }
   ],
   "source": [
    "def construct_test_set(df_for_true, df_for_false, sampleing_true, sampleing_false):\n",
    "    test_true = df_for_true[df_for_true.label == 1].sample(**sampleing_true).reset_index(drop=True)\n",
    "    test_false = df_for_false[df_for_false.label == 0].sample(**sampleing_false).reset_index(drop=True)\n",
    "    test_df = pd.concat([test_true, test_false],ignore_index=True).reset_index(drop=True)\n",
    "    return test_df\n",
    "\n",
    "def construct_train_set(df, testsamples):\n",
    "    df_train = df[~df.sentence.isin(testsamples.sentence)]\n",
    "    return df_train\n",
    "\n",
    "URLLabel_clean_counter = Counter(URLLabel_clean.label)\n",
    "print(\"URLLabel_clean_counter:\",URLLabel_clean_counter)\n",
    "# Counter({0: 1407, 1: 606}) \n",
    "false_true_ratio = URLLabel_clean_counter[0]/URLLabel_clean_counter[1]\n",
    "print(\"false_true_ratio: \", false_true_ratio)\n",
    "\n",
    "## Test dataset\n",
    "# URLLabel \n",
    "URLLabel_test = construct_test_set(URLLabel_clean,URLLabel_clean,{\"frac\": 1-split_ratio_train_test,\"random_state\" : seed},{\"frac\": 1-split_ratio_train_test,\"random_state\" : seed})\n",
    "URLLabel_test_counter = Counter(URLLabel_test.label)\n",
    "print(\"URLLabel_test_counter: \" + str(URLLabel_test_counter))\n",
    "\n",
    "# DBLP/RegEx\n",
    "DBLP_test = construct_test_set(DBLP_clean, RegEx_clean, {\"n\": URLLabel_test_counter[1],\"random_state\" : seed},{\"n\": URLLabel_test_counter[0],\"random_state\" : seed})\n",
    "DBLP_test_counter = Counter(DBLP_test.label)\n",
    "DBLP_clean_counter = Counter(DBLP_clean.label)\n",
    "print(DBLP_clean_counter)\n",
    "print(\"DBLP_test_counter: \" + str(DBLP_test_counter))\n",
    "\n",
    "# Wikidata\n",
    "n_wikidata = len(Wikidata_on_seeds_clean)\n",
    "Wikidata_test = construct_test_set(Wikidata_on_seeds_clean,RegEx_clean, {\"n\": n_wikidata,\"random_state\" : seed},{\"n\": int(n_wikidata*URLLabel_test_counter[0]/URLLabel_test_counter[1]),\"random_state\" : seed})\n",
    "Wikidata_test_counter = Counter(Wikidata_test.label)\n",
    "print(\"Wikidata_test_counter: \" + str(Wikidata_test_counter))\n",
    "Wikidata_on_seeds_clean_counter = Counter(Wikidata_on_seeds_clean.label)\n",
    "print(Wikidata_on_seeds_clean_counter)\n",
    "\n",
    "## Train dataset\n",
    "# URLLabel\n",
    "URLLabel_train = URLLabel_clean[~URLLabel_clean.sentence.isin(URLLabel_test.sentence)]\n",
    "URLLabel_train_counter = Counter(URLLabel_train.label)\n",
    "print(\"URLLabel_train_counter: \"+ str(URLLabel_train_counter))\n",
    "\n",
    "#DBLP/RegEx\n",
    "DBLP_train_true = DBLP_clean[~DBLP_clean.sentence.isin(DBLP_test.sentence)]\n",
    "DBLP_train_false = RegEx_clean[RegEx_clean.label == 0][~RegEx_clean.sentence.isin(DBLP_test.sentence)].sample(int(len(DBLP_train_true)*false_true_ratio),random_state=seed).reset_index(drop=True)\n",
    "DBLP_train = pd.concat([DBLP_train_true, DBLP_train_false],ignore_index=True).reset_index(drop=True)\n",
    "DBLP_train_counter = Counter(DBLP_train.label)\n",
    "print(\"DBLP_train_counter: \" + str(DBLP_train_counter))\n",
    "\n",
    "# Remaining sample from the RegEx\n",
    "#RegEx\n",
    "remaining_RegEx = RegEx_clean[~RegEx_clean.sentence.isin(DBLP_train.sentence)]\n",
    "RegEx_train_true = remaining_RegEx[remaining_RegEx.label == 1].sample(10*URLLabel_train_counter[1],random_state=seed)\n",
    "RegEx_train_false = remaining_RegEx[remaining_RegEx.label == 0].sample(10*URLLabel_train_counter[0],random_state=seed)\n",
    "RegEx_train = pd.concat([RegEx_train_true, RegEx_train_false],ignore_index=True).sample(frac=1,random_state=seed).reset_index(drop=True)\n",
    "RegEx_train_counter = Counter(RegEx_train.label)\n",
    "print(\"RegEx_train_counter: \"+ str(RegEx_train_counter))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "M_dataset: (Counter({0: 1126, 1: 484}), [Counter({0: 281, 1: 121}), Counter({0: 281, 1: 121}), Counter({0: 589, 1: 254})])\n",
      "D_dataset: (Counter({0: 3881, 1: 1669}), [Counter({0: 281, 1: 121}), Counter({0: 281, 1: 121}), Counter({0: 589, 1: 254})])\n",
      "R_dataset: (Counter({0: 11260, 1: 4840}), [Counter({0: 281, 1: 121}), Counter({0: 281, 1: 121}), Counter({0: 589, 1: 254})])\n"
     ]
    }
   ],
   "source": [
    "def formatDataset(df, n0, n1, seed):\n",
    "    df0 = df[df['label'] == 0].sample(n0,random_state=seed)\n",
    "    df1 = df[df['label'] == 1].sample(n1,random_state=seed)\n",
    "    return pd.concat([df0,df1],ignore_index=True).reset_index(drop=True)\n",
    "\n",
    "def traintestDict(df1, df2List):\n",
    "    return {\"train\":df1.sample(frac=1,random_state=seed).reset_index(drop=True), \n",
    "          \"test\":[df2.sample(frac=1,random_state=seed).reset_index(drop=True) for df2 in df2List]}\n",
    "\n",
    "def printCounter(dataset):\n",
    "    return Counter(dataset[\"train\"].label), [Counter(df.label) for df in dataset[\"test\"]]\n",
    "\n",
    "testset_list = [URLLabel_test, DBLP_test, Wikidata_test]\n",
    "\n",
    "M_dataset = traintestDict(URLLabel_train, testset_list)\n",
    "D_dataset = traintestDict(DBLP_train, testset_list)\n",
    "R_dataset = traintestDict(RegEx_train, testset_list)\n",
    "\n",
    "print(\"M_dataset:\", printCounter(M_dataset))\n",
    "print(\"D_dataset:\", printCounter(D_dataset))\n",
    "print(\"R_dataset:\", printCounter(R_dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import datetime\n",
    "import sklearn as sk\n",
    "import sklearn.preprocessing as pp\n",
    "\n",
    "  # Function to calculate the accuracy of our predictions vs labels\n",
    "def flat_accuracy(preds, labels):\n",
    "    pred_flat = np.argmax(preds, axis=1).flatten()\n",
    "    labels_flat = labels.flatten()\n",
    "    return np.sum(pred_flat == labels_flat) / len(labels_flat)\n",
    "\n",
    "def format_time(elapsed):\n",
    "    '''\n",
    "    Takes a time in seconds and returns a string hh:mm:ss\n",
    "    '''\n",
    "    # Round to the nearest second.\n",
    "    elapsed_rounded = int(round((elapsed)))\n",
    "\n",
    "    # Format as hh:mm:ss\n",
    "    return str(datetime.timedelta(seconds=elapsed_rounded)) \n",
    "\n",
    "def person_dataset2bert_dataset(df:pd.DataFrame, tokenizer):\n",
    "    sentences = df.sentence.values\n",
    "    labels = df.label.values\n",
    "\n",
    "  # Tokenize all of the sentences and map the tokens to thier word IDs.\n",
    "    input_ids = []\n",
    "    attention_masks = []\n",
    "\n",
    "    # For every sentence...\n",
    "    for sent in sentences:\n",
    "      # `encode_plus` will:\n",
    "      #   (1) Tokenize the sentence.\n",
    "      #   (2) Prepend the `[CLS]` token to the start.\n",
    "      #   (3) Append the `[SEP]` token to the end.\n",
    "      #   (4) Map tokens to their IDs.\n",
    "      #   (5) Pad or truncate the sentence to `max_length`\n",
    "      #   (6) Create attention masks for [PAD] tokens.\n",
    "      encoded_dict = tokenizer.encode_plus(\n",
    "                          sent,                           # Sentence to encode.\n",
    "                          add_special_tokens = True,      # Add '[CLS]' and '[SEP]'\n",
    "                          max_length = 128,               # Pad & truncate all sentences.\n",
    "                          pad_to_max_length = True,\n",
    "                          return_attention_mask = True,   # Construct attn. masks.\n",
    "                          return_tensors = 'pt',          # Return pytorch tensors.\n",
    "                    )\n",
    "\n",
    "      # Add the encoded sentence to the list.    \n",
    "      input_ids.append(encoded_dict['input_ids'])\n",
    "\n",
    "      # And its attention mask (simply differentiates padding from non-padding).\n",
    "      attention_masks.append(encoded_dict['attention_mask'])\n",
    "\n",
    "    # Convert the lists into tensors.\n",
    "    input_ids = torch.cat(input_ids, dim=0)\n",
    "    attention_masks = torch.cat(attention_masks, dim=0)\n",
    "    labels = torch.tensor(labels)\n",
    "\n",
    "    from torch.utils.data import TensorDataset, random_split\n",
    "\n",
    "    # Combine the training inputs into a TensorDataset.\n",
    "    dataset = TensorDataset(input_ids, attention_masks, labels)\n",
    "    return dataset\n",
    "\n",
    "def log2prob(logits):\n",
    "    probits = list(map(lambda l: np.exp(l)/(1+ np.exp(l)), logits))\n",
    "    return pp.normalize(probits,norm='l1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def createDataLoader(train_dataset, val_dataset,batch_size):\n",
    "    # The DataLoader needs to know our batch size for training, so we specify it \n",
    "    # here. For fine-tuning BERT on a specific task, the authors recommend a batch \n",
    "    # size of 16 or 32.\n",
    "    from torch.utils.data import DataLoader, RandomSampler, SequentialSampler\n",
    "\n",
    "    # Create the DataLoaders for our training and validation sets.\n",
    "    train_dataloader = DataLoader(\n",
    "              train_dataset,                          # The training samples.\n",
    "              sampler = RandomSampler(train_dataset), # Select batches randomly\n",
    "              batch_size = batch_size                 # Trains with this batch size.\n",
    "          )\n",
    "\n",
    "    validation_dataloader = DataLoader(\n",
    "              val_dataset,                              # The validation samples.\n",
    "              sampler = SequentialSampler(val_dataset), # Pull out batches sequentially.\n",
    "              batch_size = batch_size                   # Evaluate with this batch size.\n",
    "          )\n",
    "    return train_dataloader, validation_dataloader\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "R/R - M/M 200 4218774 days, 12:15:01\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Widget Javascript not detected.  It may not be installed or enabled properly.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2f8ef6149cb84fe28fed7cd5ba0f55b2"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Widget Javascript not detected.  It may not be installed or enabled properly.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b45c5f81842343d798807a17aba693e1"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Widget Javascript not detected.  It may not be installed or enabled properly.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cc62dd97aac04e6ebe2f6a8f82f3c56d"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "R/R - M/M 200 4318774 days, 12:17:07\n",
      "R/R - M/M 200 4418774 days, 12:18:25\n",
      "R/R - M/M 200 4518774 days, 12:19:36\n",
      "R/R - M/M 200 4618774 days, 12:20:46\n",
      "R/R - M/M 200 4718774 days, 12:21:55\n"
     ]
    }
   ],
   "source": [
    "pretrained_model_name = \"bert-base-multilingual-cased\" \n",
    "import sklearn\n",
    "from sklearn.metrics import f1_score, accuracy_score, recall_score, precision_score\n",
    "from sklearn.metrics import matthews_corrcoef\n",
    "from collections import defaultdict\n",
    "import random\n",
    "import numpy as np\n",
    "from torch.utils.data import TensorDataset, random_split\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "# import logging\n",
    "# logging.getLogger(\"transformers.tokenization_utils\").setLevel(logging.ERROR)\n",
    "# logging.getLogger(\"pytorch_pretrained_bert.tokenization\").setLevel(logging.ERROR)\n",
    "\n",
    "start_sampleNo = 200\n",
    "step_size_sampleNo = 200\n",
    "sampleNoList = list(np.arange(start_sampleNo,len(M_dataset[\"train\"]),step_size_sampleNo)) + ['max']\n",
    "seedList=list(range(42,52))\n",
    "\n",
    "results = {}\n",
    "for person_dataset_name, person_dataset in zip([\"M/M - M/M\", \"R/R - M/M\", \"DBLP/R - M/M\"],[M_dataset, R_dataset, D_dataset]):\n",
    "# for person_dataset_name, person_dataset in zip([ \"R/R - M/M\", \"DBLP/R - M/M\"],[ R_dataset, D_dataset]):\n",
    "# for person_dataset_name, person_dataset in zip([ \"DBLP/R - M/M\"],[ D_dataset]):\n",
    "# for person_dataset_name, person_dataset in zip([\"M/M - M/M\"],[M_dataset]): \n",
    "    data_set_results = {}\n",
    "    save_flag = 0\n",
    "    person_dataset_name_path = \"_\".join(\"\".join(person_dataset_name.split(\"/\")).split())\n",
    "    for sampleNo in sampleNoList:\n",
    "        seed_stats = []\n",
    "        for seed in seedList:\n",
    "            if sampleNo != 'max':\n",
    "                df_train = person_dataset[\"train\"].sample(sampleNo,replace=False,random_state=seed).reset_index(drop=True)\n",
    "            else:\n",
    "                df_train = person_dataset[\"train\"]\n",
    "\n",
    "            df_test = person_dataset[\"test\"]\n",
    "\n",
    "            sampleNoStr = str(len(df_train))\n",
    "            print(person_dataset_name + \" \" + sampleNoStr + \" \" + str(seed) + format_time(time.time()))\n",
    "\n",
    "            from transformers import BertTokenizer\n",
    "\n",
    "            # print('Loading BERT tokenizer...')\n",
    "            tokenizer = BertTokenizer.from_pretrained(pretrained_model_name, do_lower_case=False)\n",
    "\n",
    "            dataset = person_dataset2bert_dataset(df_train, tokenizer)\n",
    "\n",
    "            train_size = int(split_ratio_train_val * len(dataset))\n",
    "            val_size = len(dataset) - train_size\n",
    "\n",
    "            train_dataset, val_dataset = random_split(dataset, [train_size, val_size])\n",
    "\n",
    "            batch_size = 32\n",
    "            train_dataloader, validation_dataloader = createDataLoader(train_dataset, val_dataset,batch_size)\n",
    "            from transformers import BertForSequenceClassification, BertConfig\n",
    "\n",
    "            model = BertForSequenceClassification.from_pretrained(\n",
    "              pretrained_model_name, # Use the 12-layer BERT model, with an uncased vocab.\n",
    "              num_labels = 2, # The number of output labels--2 for binary classification.\n",
    "                              # You can increase this for multi-class tasks.   \n",
    "              output_attentions = False, # Whether the model returns attentions weights.\n",
    "              output_hidden_states = False, # Whether the model returns all hidden-states.\n",
    "            )\n",
    "\n",
    "            # Tell pytorch to run this model on the GPU.\n",
    "            model.cuda()\n",
    "\n",
    "            from torch import optim\n",
    "            optimizer = optim.AdamW(model.parameters(), lr = 2e-5, # args.learning_rate - default is 5e-5, our notebook had 2e-5\n",
    "                              eps = 1e-8 # args.adam_epsilon  - default is 1e-8.\n",
    "                            )\n",
    "\n",
    "            from transformers import get_linear_schedule_with_warmup\n",
    "\n",
    "            epochs = 4\n",
    "            total_steps = len(train_dataloader) * epochs\n",
    "\n",
    "            scheduler = get_linear_schedule_with_warmup(optimizer, \n",
    "                                                      num_warmup_steps = 0, # Default value in run_glue.py\n",
    "                                                      num_training_steps = total_steps)\n",
    "\n",
    "\n",
    "            # This training code is based on the `run_glue.py` script here:\n",
    "            # https://github.com/huggingface/transformers/blob/5bfcd0485ece086ebcbed2d008813037968a9e58/examples/run_glue.py#L128\n",
    "\n",
    "            seed_val = seed\n",
    "\n",
    "            random.seed(seed_val)\n",
    "            np.random.seed(seed_val)\n",
    "            torch.manual_seed(seed_val)\n",
    "            torch.cuda.manual_seed_all(seed_val)\n",
    "\n",
    "            training_stats = []\n",
    "            predict_stats = []\n",
    "\n",
    "            total_t0 = time.time()\n",
    "            total_steps = 0\n",
    "            # For each epoch...\n",
    "            for epoch_i in range(0, epochs):\n",
    "                    \n",
    "              # ========================================\n",
    "              #               Training\n",
    "              # ========================================\n",
    "\n",
    "              # Measure how long the training epoch takes.\n",
    "                t0 = time.time()\n",
    "\n",
    "                # Reset the total loss for this epoch.\n",
    "                total_train_loss = 0\n",
    "                model.train()\n",
    "\n",
    "                # For each batch of training data...\n",
    "                for step, batch in enumerate(train_dataloader):\n",
    "\n",
    "                    total_steps+=1\n",
    "\n",
    "                    if step % 40 == 0 and not step == 0:\n",
    "                        elapsed = format_time(time.time() - t0)\n",
    "                      # print('  Batch {:>5,}  of  {:>5,}.    Elapsed: {:}.'.format(step, len(train_dataloader), elapsed))\n",
    "\n",
    "                    b_input_ids = batch[0].to(device)\n",
    "                    b_input_mask = batch[1].to(device)\n",
    "                    b_labels = batch[2].to(device)\n",
    "\n",
    "                    model.zero_grad()        \n",
    "\n",
    "                    loss_logits = model(b_input_ids, \n",
    "                                      token_type_ids=None, \n",
    "                                      attention_mask=b_input_mask, \n",
    "                                      labels=b_labels)\n",
    "\n",
    "                    loss = loss_logits[0]\n",
    "                    logits = loss_logits[1]\n",
    "\n",
    "                    total_train_loss += loss.item()\n",
    "\n",
    "                    loss.backward()\n",
    "\n",
    "                    torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "\n",
    "                    optimizer.step()\n",
    "                    scheduler.step()\n",
    "\n",
    "                avg_train_loss = total_train_loss / len(train_dataloader)            \n",
    "                training_time = format_time(time.time() - t0)\n",
    "              \n",
    "              # ========================================\n",
    "              #               Validation\n",
    "              # ========================================\n",
    "\n",
    "\n",
    "                t0 = time.time()\n",
    "\n",
    "                model.eval()\n",
    "\n",
    "                # Tracking variables \n",
    "                total_eval_accuracy = 0\n",
    "                total_eval_loss = 0\n",
    "                nb_eval_steps = 0\n",
    "                total_eval_auc_roc = []\n",
    "                total_eval_f1_score = []\n",
    "                total_eval_recall = []\n",
    "                total_eval_prec = []\n",
    "                y_true = []\n",
    "                y_pred = []\n",
    "\n",
    "              # Evaluate data for one epoch\n",
    "                for batch in validation_dataloader:\n",
    "\n",
    "                    b_input_ids = batch[0].to(device)\n",
    "                    b_input_mask = batch[1].to(device)\n",
    "                    b_labels = batch[2].to(device)\n",
    "\n",
    "                    # Tell pytorch not to bother with constructing the compute graph during\n",
    "                    # the forward pass, since this is only needed for backprop (training).\n",
    "                    with torch.no_grad():        \n",
    "\n",
    "                        loss_logits = model(b_input_ids, \n",
    "                                            token_type_ids=None, \n",
    "                                            attention_mask=b_input_mask,\n",
    "                                            labels=b_labels)\n",
    "\n",
    "                        loss = loss_logits[0]\n",
    "                        logits = loss_logits[1]\n",
    "\n",
    "                    total_eval_loss += loss.item()\n",
    "\n",
    "                    # Move logits and labels to CPU\n",
    "                    logits = logits.detach().cpu().numpy()\n",
    "                    label_ids = b_labels.to('cpu').numpy()\n",
    "\n",
    "                    # Determine probability\n",
    "                    probits = log2prob(logits)\n",
    "\n",
    "                    # Calculate the accuracy for this batch of test sentences, and\n",
    "                    # accumulate it over all batches.\n",
    "                    y_true.append(label_ids)\n",
    "                    y_pred.append(probits.T[1])\n",
    "\n",
    "                    total_eval_accuracy += flat_accuracy(logits, label_ids)\n",
    "                    \n",
    "                flat_true_labels = np.concatenate(y_true, axis=0)\n",
    "\n",
    "                flat_probs = np.concatenate(y_pred, axis=0)\n",
    "\n",
    "                # Combine the correct labels for each batch into a single list.\n",
    "                y_true = flat_true_labels\n",
    "                y_pred = flat_probs\n",
    "\n",
    "                # total_eval_auc_roc.append(sklearn.metrics.roc_auc_score(y_true, y_pred))\n",
    "                total_eval_f1_score.append(f1_score(y_true,np.round(y_pred)))\n",
    "                total_eval_recall.append(recall_score(y_true,np.round(y_pred)))\n",
    "                total_eval_prec.append(precision_score(y_true,np.round(y_pred)))\n",
    "\n",
    "                # Report the final accuracy for this validation run.\n",
    "                avg_val_accuracy = total_eval_accuracy / len(validation_dataloader)\n",
    "                # print(\"  Accuracy: {0:.2f}\".format(avg_val_accuracy))\n",
    "\n",
    "                # Calculate the average loss over all of the batches.\n",
    "                avg_val_loss = total_eval_loss / len(validation_dataloader)\n",
    "              \n",
    "                # Measure how long the validation run took.\n",
    "                validation_time = format_time(time.time() - t0)\n",
    "              \n",
    "                # print(\"  Validation Loss: {0:.2f}\".format(avg_val_loss))\n",
    "                # print(\"  Validation took: {:}\".format(validation_time))\n",
    "\n",
    "                # Record all statistics from this epoch.\n",
    "                val_stats_dict = {   'epoch': epoch_i + 1,\n",
    "                                'total_steps': total_steps,\n",
    "                      'Training Loss': avg_train_loss,\n",
    "                      # 'Valid. avg. AUC': np.average(total_eval_auc_roc),\n",
    "                      'Valid. avg. F1' : np.average(total_eval_f1_score),\n",
    "                      'Valid. Recall': np.average(total_eval_recall),\n",
    "                      'Valid. Prec.' : np.average(total_eval_prec),\n",
    "                      'Valid. Loss': avg_val_loss,\n",
    "                      'Valid. Accur.': avg_val_accuracy,\n",
    "                      'Training Time': training_time,\n",
    "                      'Validation Time': validation_time\n",
    "                  }\n",
    "                training_stats.append( val_stats_dict\n",
    "                )\n",
    "\n",
    "                for testset, df_predict in enumerate(df_test):\n",
    "                              # df_predict = df_test\n",
    "                    # Report the number of sentences.\n",
    "                    from torch.utils.data import DataLoader, RandomSampler, SequentialSampler\n",
    "\n",
    "                    # print('Number of test sentences: {:,}\\n'.format(df_predict.shape[0]))\n",
    "                    prediction_data = person_dataset2bert_dataset(df_predict, tokenizer)\n",
    "                    prediction_sampler = SequentialSampler(prediction_data)\n",
    "                    prediction_dataloader = DataLoader(prediction_data, sampler=prediction_sampler, batch_size=batch_size)\n",
    "\n",
    "                    # print('Predicting labels for {:,} test sentences...'.format(len(input_ids)))\n",
    "\n",
    "                    # Put model in evaluation mode\n",
    "                    model.eval()\n",
    "\n",
    "                    # Tracking variables \n",
    "                    predictions , true_labels = [], []\n",
    "                    probs = []\n",
    "\n",
    "                    # Predict \n",
    "                    for batch in prediction_dataloader:\n",
    "                            # Add batch to GPU\n",
    "                            batch = tuple(t.to(device) for t in batch)\n",
    "\n",
    "                            # Unpack the inputs from our dataloader\n",
    "                            b_input_ids, b_input_mask, b_labels  = batch\n",
    "\n",
    "                            # Telling the model not to compute or store gradients, saving memory and \n",
    "                            # speeding up prediction\n",
    "                            with torch.no_grad():\n",
    "                              # Forward pass, calculate logit predictions\n",
    "                              outputs = model(b_input_ids, token_type_ids=None, \n",
    "                                              attention_mask=b_input_mask)\n",
    "\n",
    "                            logits = outputs[0]\n",
    "\n",
    "                            # Move logits and labels to CPU\n",
    "                            logits = logits.detach().cpu().numpy()\n",
    "                            label_ids = b_labels.to('cpu').numpy()\n",
    "\n",
    "                            # Probits\n",
    "                            probits = log2prob(logits)\n",
    "\n",
    "                            # Store predictions and true labels\n",
    "                            predictions.append(logits)\n",
    "                            true_labels.append(label_ids)\n",
    "                            probs.append(probits.T[1])\n",
    "\n",
    "\n",
    "                    flat_predictions = np.concatenate(predictions, axis=0)\n",
    "                    flat_probs = np.concatenate(probs, axis=0)\n",
    "                    # For each sample, pick the label (0 or 1) with the higher score.\n",
    "                    flat_predictions = np.argmax(flat_predictions, axis=1).flatten()\n",
    "\n",
    "                    # Combine the correct labels for each batch into a single list.\n",
    "                    flat_true_labels = np.concatenate(true_labels, axis=0)\n",
    "\n",
    "                    predict_stats.append(\n",
    "                        {   'TestSet':testset,\n",
    "                            'epoch' : epoch_i + 1,\n",
    "                            'total_steps': total_steps,\n",
    "                            'Test AUC'   : sklearn.metrics.roc_auc_score(flat_true_labels, flat_probs),\n",
    "                            'Test F1'    : f1_score(flat_true_labels, flat_predictions, average='macro'),\n",
    "                            'Test Accu.' : accuracy_score(flat_true_labels, flat_predictions),\n",
    "                            'Test Recall': recall_score(flat_true_labels, flat_predictions),\n",
    "                            'Test Prec.' : precision_score(flat_true_labels, flat_predictions),\n",
    "                            'MCC.'  : matthews_corrcoef(flat_true_labels, flat_predictions),\n",
    "                            # 'True_Labels' : flat_true_labels,\n",
    "                            # 'Predictions' : flat_predictions,\n",
    "                        }\n",
    "                    )\n",
    "\n",
    "              # if (sampleNo == 'max') & (save_flag == 0):\n",
    "              #   save_flag = 1\n",
    "              #   torch.save(model.state_dict(), '/content/gdrive/MyDrive/InstituteClustering/bert/models/with_predictions_seeded_'+ person_dataset_name_path)\n",
    "     \n",
    "            seed_stats.append({str(seed) : { \"train_stats\": training_stats , \"predict_stats\":predict_stats}})\n",
    "        data_set_results[sampleNoStr] = seed_stats\n",
    "    results[person_dataset_name] = data_set_results\n",
    "    import json\n",
    "    person_dataset_name_path = \"_\".join(\"\".join(person_dataset_name.split(\"/\")).split())\n",
    "    with open('/content/gdrive/MyDrive/InstituteClustering/bert/results/camera_ready_with_predictions_seeded_' + person_dataset_name_path +'.json', 'w') as fp:\n",
    "        json.dump(data_set_results, fp) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
